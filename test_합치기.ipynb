{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.layers import Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating=pd.read_csv('all_data_final.csv', encoding='utf-8')\n",
    "rating=rating.iloc[:,[0,1,2,5,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]]\n",
    "rating=rating.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=rating[rating['dominant_R']!=0]\n",
    "none_rgb=rating[rating.dominant_R==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/na00mi/.local/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/na00mi/.local/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#MF data processing\n",
    "# User encoding\n",
    "user_dict = {}\n",
    "for i in set(ratings['User-ID']):\n",
    "    user_dict[i] = len(user_dict)\n",
    "n_user = len(user_dict)\n",
    "\n",
    "# Item encoding\n",
    "item_dict = {}\n",
    "start_point = n_user\n",
    "for i in set(ratings['ISBN']):\n",
    "    item_dict[i] = start_point + len(item_dict)\n",
    "n_item = len(item_dict)\n",
    "start_point += n_item\n",
    "\n",
    "# author rating group2 encoding\n",
    "author_dict = {}\n",
    "for i in set(ratings['Author_3cluster']):\n",
    "    author_dict[i] = start_point + len(author_dict)\n",
    "n_author = len(author_dict)\n",
    "start_point += n_author\n",
    "\n",
    "dominant_R_index=start_point\n",
    "start_point += 1\n",
    "\n",
    "dominant_G_index=start_point\n",
    "start_point += 1\n",
    "\n",
    "dominant_B_index=start_point\n",
    "start_point += 1\n",
    "\n",
    "num_x = start_point             # Total number of x\n",
    "\n",
    "# DL data processing\n",
    "ISBN_dict = dict()\n",
    "\n",
    "for i, ISBN in enumerate(none_rgb['ISBN'].unique()):\n",
    "    ISBN_dict[ISBN] = i\n",
    "none_rgb['ISBN_idx'] = none_rgb['ISBN'].apply(lambda x : ISBN_dict[x])\n",
    "\n",
    "# make User-ID to int\n",
    "USER_dict = dict()\n",
    "\n",
    "for i, USER in enumerate(none_rgb['User-ID'].unique()):\n",
    "    USER_dict[USER] = i\n",
    "none_rgb['USER_idx'] = none_rgb['User-ID'].apply(lambda x : USER_dict[x])\n",
    "\n",
    "r_cols = ['USER_idx', 'ISBN_idx', 'Book-Rating']\n",
    "u_cols = ['USER_idx', 'Age-Group']\n",
    "b_cols = ['ISBN_idx', 'Author_3cluster']\n",
    "y_cols=['ISBN_idx', 'Pub_Year_Group']\n",
    "\n",
    "book_ratings = none_rgb[r_cols]\n",
    "users = none_rgb[u_cols].drop_duplicates()\n",
    "books = none_rgb[b_cols].drop_duplicates()\n",
    "pub_years=none_rgb[y_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test 분리\n",
    "TRAIN_SIZE = 0.7\n",
    "ratings = shuffle(ratings)\n",
    "\n",
    "cutoff = int(TRAIN_SIZE * len(ratings))\n",
    "ratings_train = ratings.iloc[:cutoff]\n",
    "ratings_test = ratings.iloc[cutoff:]\n",
    "x = shuffle(ratings_train, random_state=1)\n",
    "x_test = shuffle(ratings_test, random_state=1)\n",
    "\n",
    "none_rgb = shuffle(none_rgb)\n",
    "cutoff = int(TRAIN_SIZE * len(none_rgb))\n",
    "none_rgb_train = none_rgb.iloc[:cutoff]\n",
    "none_rgb_test = none_rgb.iloc[cutoff:]\n",
    "\n",
    "test_data=pd.concat([x_test, none_rgb_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding  0  cases...\n",
      "Encoding  10000  cases...\n"
     ]
    }
   ],
   "source": [
    "#FM Generate X data\n",
    "data = []\n",
    "y = []\n",
    "\n",
    "R_mean = np.mean(x['dominant_R'])\n",
    "R_std = np.std(x['dominant_R'])\n",
    "G_mean = np.mean(x['dominant_G'])\n",
    "G_std = np.std(x['dominant_G'])\n",
    "B_mean = np.mean(x['dominant_B'])\n",
    "B_std = np.std(x['dominant_B'])\n",
    "\n",
    "w0 = np.mean(x['Book-Rating'])\n",
    "for i in range(len(x)):\n",
    "    case = x.iloc[i]\n",
    "    x_index = []\n",
    "    x_value = []\n",
    "    x_index.append(user_dict[case['User-ID']])     # User id encoding\n",
    "    x_value.append(1.)\n",
    "    x_index.append(item_dict[case['ISBN']])    # Movie id encoding\n",
    "    x_value.append(1.)\n",
    "    x_index.append(author_dict[case['Author_3cluster']])   # author group id encoding\n",
    "    x_value.append(1.)\n",
    "    x_index.append(dominant_R_index)\n",
    "    x_value.append((case['dominant_R']-R_mean)/R_std)\n",
    "    x_index.append(dominant_G_index)\n",
    "    x_value.append((case['dominant_G']-G_mean)/G_std)\n",
    "    x_index.append(dominant_B_index)\n",
    "    x_value.append((case['dominant_B']-B_mean)/B_std)\n",
    "\n",
    "    data.append([x_index, x_value])\n",
    "    y.append(case['Book-Rating'] - w0)\n",
    "    if (i % 10000) == 0:\n",
    "        print('Encoding ', i, ' cases...')\n",
    "        \n",
    "        \n",
    "#DL generate data\n",
    "##GPU : 특정 GPU만 사용할 수 있도록##\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 세 번째 GPU만 사용하도록 제한\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU') #gpus 조정(0~3)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)\n",
    "\n",
    "# Keras model\n",
    "##GPU : 특정 GPU만 사용할 수 있도록##\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 세 번째 GPU만 사용하도록 제한\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU') #gpus 조정(0~3)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)\n",
    "\n",
    "train_u = pd.merge(none_rgb_train, users, on = 'USER_idx')['Age-Group_x']\n",
    "test_u = pd.merge(none_rgb_test, users, on = 'USER_idx')['Age-Group_x']\n",
    "\n",
    "train_b = pd.merge(none_rgb_train, books, on = 'ISBN_idx')['Author_3cluster_x']\n",
    "test_b = pd.merge(none_rgb_test, books, on = 'ISBN_idx')['Author_3cluster_x']\n",
    "\n",
    "train_y = pd.merge(none_rgb_train, pub_years, on = 'ISBN_idx')['Pub_Year_Group_x']\n",
    "test_y = pd.merge(none_rgb_test, pub_years, on = 'ISBN_idx')['Pub_Year_Group_x']\n",
    "\n",
    "uL = len(users['Age-Group'])\n",
    "aL = len(books['Author_3cluster'])\n",
    "yL = len(pub_years['Pub_Year_Group'])\n",
    "# Variable 초기화 \n",
    "# Variable 초기화 \n",
    "K = 20                             # Latent factor 수 \n",
    "reg = 0.0001                        # Regularization penalty\n",
    "mu = none_rgb_train['Book-Rating'].mean()    # 전체 평균 \n",
    "M = none_rgb.USER_idx.max() + 1    # Number of users\n",
    "N = none_rgb.ISBN_idx.max() + 1    # Number of movies\n",
    "\n",
    "\n",
    "user = Input(shape=(1, ))\n",
    "item = Input(shape=(1, ))\n",
    "P_embedding = Embedding(M, K, embeddings_regularizer=l2(reg))(user) # 20\n",
    "Q_embedding = Embedding(N, K, embeddings_regularizer=l2(reg))(item) # 20 + 20 = 40\n",
    "user_bias = Embedding(M, 1, embeddings_regularizer=l2(reg))(user) # 40 + 1 = 41\n",
    "item_bias = Embedding(N, 1, embeddings_regularizer=l2(reg))(item) # 41 + 1 = 42\n",
    "\n",
    "# Concatenate layers\n",
    "P_embedding = Flatten()(P_embedding)\n",
    "Q_embedding = Flatten()(Q_embedding)\n",
    "user_bias = Flatten()(user_bias)\n",
    "item_bias = Flatten()(item_bias)\n",
    "\n",
    "age = Input(shape=(1, ))\n",
    "age_embedding = Embedding(uL, 3, embeddings_regularizer = l2())(age) # 42 + 1 = 43\n",
    "age_layer = Flatten()(age_embedding)\n",
    "\n",
    "author = Input(shape=(1, ))\n",
    "author_embedding = Embedding(aL, 3, embeddings_regularizer = l2())(author) # 43 + 1 = 44\n",
    "author_layer = Flatten()(author_embedding)\n",
    "\n",
    "year = Input(shape=(1, ))\n",
    "year_embedding = Embedding(yL, 3, embeddings_regularizer = l2())(year) # 43 + 1 = 44\n",
    "year_layer = Flatten()(year_embedding)\n",
    "\n",
    "R = Concatenate()([P_embedding, Q_embedding, user_bias, item_bias, age_layer, author_layer, year_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining RMSE measure\n",
    "def RMSE(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "def RMSE2(y_true, y_pred):\n",
    "      return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred))**2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FM model\n",
    "class FM():\n",
    "    def __init__(self, N, K, data, y, alpha, beta, train_ratio=0.75, iterations=100, tolerance=0.005, l2_reg=True, verbose=True):\n",
    "        self.K = K          # Number of latent factors\n",
    "        self.N = N          # Number of x (variables)\n",
    "        self.n_cases = len(data)            # N of observations\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.l2_reg = l2_reg\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "        # w 초기화\n",
    "        self.w = np.random.normal(scale=1./self.N, size=(self.N))\n",
    "        # v 초기화\n",
    "        self.v = np.random.normal(scale=1./self.K, size=(self.N, self.K))\n",
    "        # Train/Test 분리\n",
    "        cutoff = int(train_ratio * len(data))\n",
    "        self.train_x = data[:cutoff]\n",
    "        self.test_x = data[cutoff:]\n",
    "        self.train_y = y[:cutoff]\n",
    "        self.test_y = y[cutoff:]\n",
    "\n",
    "    def test(self):                                     # Training 하면서 RMSE 계산 \n",
    "        # SGD를 iterations 숫자만큼 수행\n",
    "        best_RMSE = 10000\n",
    "        best_iteration = 0\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            rmse1 = self.sgd(self.train_x, self.train_y)        # SGD & Train RMSE 계산\n",
    "            rmse2 = self.test_rmse(self.test_x, self.test_y)    # Test RMSE 계산     \n",
    "            training_process.append((i, rmse1, rmse2))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.6f ; Test RMSE = %.6f\" % (i+1, rmse1, rmse2))\n",
    "            if best_RMSE > rmse2:                       # New best record\n",
    "                best_RMSE = rmse2\n",
    "                best_iteration = i\n",
    "            elif (rmse2 - best_RMSE) > self.tolerance:  # RMSE is increasing over tolerance\n",
    "                break\n",
    "        print(best_iteration, best_RMSE)\n",
    "        return training_process\n",
    "        \n",
    "    # w, v 업데이트를 위한 Stochastic gradient descent \n",
    "    def sgd(self, x_data, y_data):\n",
    "        y_pred = []\n",
    "        for data, y in zip(x_data, y_data):\n",
    "            x_idx = data[0]\n",
    "            x_0 = np.array(data[1])     # xi axis=0 [1, 2, 3]\n",
    "            x_1 = x_0.reshape(-1, 1)    # xi axis=1 [[1], [2], [3]]\n",
    "    \n",
    "            # biases\n",
    "            bias_score = np.sum(self.w[x_idx] * x_0)\n",
    "    \n",
    "            # score 계산\n",
    "            vx = self.v[x_idx] * (x_1)\n",
    "            sum_vx = np.sum(vx, axis=0)\n",
    "            sum_vx_2 = np.sum(vx * vx, axis=0)\n",
    "            latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "\n",
    "            # 예측값 계산\n",
    "            y_hat = bias_score + latent_score\n",
    "            y_pred.append(y_hat)\n",
    "            error = y - y_hat\n",
    "            # w, v 업데이트\n",
    "            if self.l2_reg:\n",
    "                self.w[x_idx] += error * self.alpha * (x_0 - self.beta * self.w[x_idx])\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx * x_1) - self.beta * self.v[x_idx])\n",
    "            else:\n",
    "                self.w[x_idx] += error * self.alpha * x_0\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx * x_1))\n",
    "        return RMSE2(y_data, y_pred)\n",
    "            \n",
    "    def test_rmse(self, x_data, y_data):\n",
    "        y_pred = []\n",
    "        for data , y in zip(x_data, y_data):\n",
    "            y_hat = self.predict(data[0], data[1])\n",
    "            y_pred.append(y_hat)\n",
    "        return RMSE2(y_data, y_pred)\n",
    "    \n",
    "    #hybrid 예측을 위해서 추가\n",
    "    def test_rmse2(self, x_data, y_data):\n",
    "        y_hat = self.predict(x_data[0], x_data[1])\n",
    "        return y_data, y_hat\n",
    "\n",
    "    def predict(self, idx, x):\n",
    "        x_0 = np.array(x)\n",
    "        x_1 = x_0.reshape(-1, 1)\n",
    "\n",
    "        # biases\n",
    "        bias_score = np.sum(self.w[idx] * x_0)\n",
    "\n",
    "        # score 계산\n",
    "        vx = self.v[idx] * (x_1)\n",
    "        sum_vx = np.sum(vx, axis=0)\n",
    "        sum_vx_2 = np.sum(vx * vx, axis=0)\n",
    "        latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "\n",
    "        # 예측값 계산\n",
    "        y_hat = bias_score + latent_score\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL model\n",
    "R = Dense(2048)(R)\n",
    "R = Activation('relu')(R)\n",
    "R = Dense(512)(R)\n",
    "R = Activation('linear')(R)\n",
    "R = Dense(1)(R)\n",
    "\n",
    "model = Model(inputs=[user, item, age, author, year], outputs=R)\n",
    "model.compile(\n",
    "  loss=RMSE,\n",
    "  optimizer=Adam(lr=0.001),\n",
    "  metrics=[RMSE]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 1.619285 ; Test RMSE = 1.612121\n",
      "Iteration: 20 ; Train RMSE = 1.536150 ; Test RMSE = 1.526587\n",
      "Iteration: 30 ; Train RMSE = 1.506927 ; Test RMSE = 1.494261\n",
      "Iteration: 40 ; Train RMSE = 1.496288 ; Test RMSE = 1.481489\n",
      "Iteration: 50 ; Train RMSE = 1.491880 ; Test RMSE = 1.475969\n",
      "Iteration: 60 ; Train RMSE = 1.489541 ; Test RMSE = 1.473254\n",
      "Iteration: 70 ; Train RMSE = 1.487887 ; Test RMSE = 1.471697\n",
      "Iteration: 80 ; Train RMSE = 1.486460 ; Test RMSE = 1.470654\n",
      "Iteration: 90 ; Train RMSE = 1.485107 ; Test RMSE = 1.469858\n",
      "Iteration: 100 ; Train RMSE = 1.483779 ; Test RMSE = 1.469188\n",
      "99 1.4691875048945375\n"
     ]
    }
   ],
   "source": [
    "#train FM\n",
    "K = 200\n",
    "fm1 = FM(num_x, K, data, y, alpha=0.000025, beta=0.007, train_ratio=0.8, iterations=100, tolerance=0.0001, l2_reg=True, verbose=True)\n",
    "result = fm1.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "56/56 [==============================] - 1s 11ms/step - loss: 2.2429 - RMSE: 1.8454 - val_loss: 1.6906 - val_RMSE: 1.6801\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 1.4450 - RMSE: 1.4361 - val_loss: 1.7506 - val_RMSE: 1.7422\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 1.0863 - RMSE: 1.0771 - val_loss: 1.7538 - val_RMSE: 1.7442\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.8951 - RMSE: 0.8846 - val_loss: 1.7551 - val_RMSE: 1.7443\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.7900 - RMSE: 0.7784 - val_loss: 1.8072 - val_RMSE: 1.7955\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.7199 - RMSE: 0.7071 - val_loss: 1.8148 - val_RMSE: 1.8016\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.6711 - RMSE: 0.6572 - val_loss: 1.8320 - val_RMSE: 1.8176\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.6273 - RMSE: 0.6123 - val_loss: 1.8836 - val_RMSE: 1.8684\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.5641 - RMSE: 0.5481 - val_loss: 1.8640 - val_RMSE: 1.8474\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.5197 - RMSE: 0.5028 - val_loss: 1.8441 - val_RMSE: 1.8266\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.4669 - RMSE: 0.4491 - val_loss: 1.8662 - val_RMSE: 1.8476\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.4225 - RMSE: 0.4037 - val_loss: 1.8709 - val_RMSE: 1.8514\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.3832 - RMSE: 0.3636 - val_loss: 1.8709 - val_RMSE: 1.8505\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.3578 - RMSE: 0.3375 - val_loss: 1.8382 - val_RMSE: 1.8171\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.3391 - RMSE: 0.3180 - val_loss: 1.8418 - val_RMSE: 1.8200\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.3151 - RMSE: 0.2933 - val_loss: 1.8595 - val_RMSE: 1.8369\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.3077 - RMSE: 0.2852 - val_loss: 1.8472 - val_RMSE: 1.8239\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2810 - RMSE: 0.2579 - val_loss: 1.8617 - val_RMSE: 1.8380\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2753 - RMSE: 0.2516 - val_loss: 1.8528 - val_RMSE: 1.8284\n",
      "Epoch 20/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2527 - RMSE: 0.2284 - val_loss: 1.8625 - val_RMSE: 1.8378\n",
      "Epoch 21/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2539 - RMSE: 0.2291 - val_loss: 1.8354 - val_RMSE: 1.8100\n",
      "Epoch 22/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2499 - RMSE: 0.2246 - val_loss: 1.8358 - val_RMSE: 1.8101\n",
      "Epoch 23/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2479 - RMSE: 0.2221 - val_loss: 1.8271 - val_RMSE: 1.8008\n",
      "Epoch 24/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2319 - RMSE: 0.2057 - val_loss: 1.8497 - val_RMSE: 1.8230\n",
      "Epoch 25/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2354 - RMSE: 0.2087 - val_loss: 1.8395 - val_RMSE: 1.8125\n",
      "Epoch 26/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2246 - RMSE: 0.1975 - val_loss: 1.8448 - val_RMSE: 1.8173\n",
      "Epoch 27/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2135 - RMSE: 0.1860 - val_loss: 1.8678 - val_RMSE: 1.8400\n",
      "Epoch 28/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2217 - RMSE: 0.1938 - val_loss: 1.8378 - val_RMSE: 1.8097\n",
      "Epoch 29/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2129 - RMSE: 0.1847 - val_loss: 1.8428 - val_RMSE: 1.8143\n",
      "Epoch 30/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2058 - RMSE: 0.1773 - val_loss: 1.8472 - val_RMSE: 1.8183\n",
      "Epoch 31/100\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.2104 - RMSE: 0.1815 - val_loss: 1.8453 - val_RMSE: 1.8161\n",
      "Epoch 00031: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "\n",
    "result = model.fit(\n",
    "  x=[none_rgb_train['USER_idx'].values, none_rgb_train['ISBN_idx'].values, train_u.values, train_b.values, train_y.values],\n",
    "  y=none_rgb_train['Book-Rating'].values - mu,\n",
    "  epochs=100,\n",
    "  batch_size=256,\n",
    "  validation_data=(\n",
    "    [none_rgb_test['USER_idx'].values, none_rgb_test['ISBN_idx'].values, test_u.values, test_b.values, test_y.values],\n",
    "    none_rgb_test['Book-Rating'].values - mu\n",
    "  ),\n",
    "  callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "fm_predictions=[]\n",
    "dl_predictions=[]\n",
    "weight=[0.3,0.7]\n",
    "for i in range(len(test_data)):\n",
    "    case=test_data.iloc[i]\n",
    "    \n",
    "    if case['dominant_R']==0.0:\n",
    "        dl_predictions.append(case)\n",
    "        \n",
    "    else:\n",
    "        x_index = []\n",
    "        x_value = []\n",
    "        x_index.append(user_dict[case['User-ID']])     # User id encoding\n",
    "        x_value.append(1.)\n",
    "        x_index.append(item_dict[case['ISBN']])    # Movie id encoding\n",
    "        x_value.append(1.)\n",
    "        x_index.append(author_dict[case['Author_3cluster']])   # author group id encoding\n",
    "        x_value.append(1.)\n",
    "        x_index.append(dominant_R_index)\n",
    "        x_value.append((case['dominant_R']-R_mean)/R_std)\n",
    "        x_index.append(dominant_G_index)\n",
    "        x_value.append((case['dominant_G']-G_mean)/G_std)\n",
    "        x_index.append(dominant_B_index)\n",
    "        x_value.append((case['dominant_B']-B_mean)/B_std)\n",
    "        \n",
    "        x_data=[x_index, x_value]\n",
    "        y_data=case['Book-Rating'] - w0\n",
    "        y_true, y_pred=fm1.test_rmse2(x_data, y_data)\n",
    "        temp=[y_true*weight[0],y_pred*weight[0]]\n",
    "        fm_predictions.append(temp)\n",
    "        \n",
    "fm_predictions=np.array(fm_predictions).transpose()\n",
    "fm_y_pred=fm_predictions[0]\n",
    "fm_y_true=fm_predictions[1]\n",
    "       \n",
    "dl_predictions=pd.DataFrame(dl_predictions) \n",
    "user_ids = dl_predictions['USER_idx'].values\n",
    "item_ids = dl_predictions['ISBN_idx'].values\n",
    "age_ids = dl_predictions['Age-Group'].values\n",
    "author_ids = dl_predictions['Author_3cluster'].values\n",
    "year_ids=dl_predictions['Pub_Year_Group'].values\n",
    "y_pred = model.predict([user_ids, item_ids, test_u, test_b, test_y]) + mu\n",
    "y_pred = np.ravel(y_pred, order='C') * weight[1]\n",
    "y_true = np.array(dl_predictions['Book-Rating']) * weight[1]\n",
    "\n",
    "all_y_pred=np.concatenate((fm_y_pred,y_pred),axis=0)\n",
    "all_y_true=np.concatenate((fm_y_true,y_true),axis=0)\n",
    "\n",
    "res_rmse=RMSE2(all_y_pred, all_y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0097066077565433"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "fm_predictions=[]\n",
    "dl_predictions=[]\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    case=test_data.iloc[i]\n",
    "    \n",
    "    if case['dominant_R']==0.0:\n",
    "        dl_predictions.append(case)\n",
    "        \n",
    "    else:\n",
    "        x_index = []\n",
    "        x_value = []\n",
    "        x_index.append(user_dict[case['User-ID']])     # User id encoding\n",
    "        x_value.append(1.)\n",
    "        x_index.append(item_dict[case['ISBN']])    # Movie id encoding\n",
    "        x_value.append(1.)\n",
    "        x_index.append(author_dict[case['Author_3cluster']])   # author group id encoding\n",
    "        x_value.append(1.)\n",
    "        x_index.append(dominant_R_index)\n",
    "        x_value.append((case['dominant_R']-R_mean)/R_std)\n",
    "        x_index.append(dominant_G_index)\n",
    "        x_value.append((case['dominant_G']-G_mean)/G_std)\n",
    "        x_index.append(dominant_B_index)\n",
    "        x_value.append((case['dominant_B']-B_mean)/B_std)\n",
    "        \n",
    "        x_data=[x_index, x_value]\n",
    "        y_data=case['Book-Rating'] - w0\n",
    "        y_true, y_pred=fm1.test_rmse2(x_data, y_data)\n",
    "        temp=[y_true,y_pred]\n",
    "        fm_predictions.append(temp)\n",
    "        \n",
    "#rmse 계산 형태 만들어주기        \n",
    "fm_predictions=np.array(fm_predictions).transpose()\n",
    "fm_y_pred=fm_predictions[0]\n",
    "fm_y_true=fm_predictions[1]\n",
    "\n",
    "#dl 모델의 예측을 위한 y_pred, y_true 계산       \n",
    "dl_predictions=pd.DataFrame(dl_predictions) \n",
    "user_ids = dl_predictions['USER_idx'].values\n",
    "item_ids = dl_predictions['ISBN_idx'].values\n",
    "age_ids = dl_predictions['Age-Group'].values\n",
    "author_ids = dl_predictions['Author_3cluster'].values\n",
    "year_ids=dl_predictions['Pub_Year_Group'].values\n",
    "y_pred = model.predict([user_ids, item_ids, test_u, test_b, test_y]) + mu\n",
    "y_pred = np.ravel(y_pred, order='C') \n",
    "y_true = np.array(dl_predictions['Book-Rating']) \n",
    "\n",
    "#fm+dl\n",
    "all_y_pred=np.concatenate((fm_y_pred,y_pred),axis=0)\n",
    "all_y_true=np.concatenate((fm_y_true,y_true),axis=0)\n",
    "\n",
    "res_rmse=RMSE2(all_y_pred, all_y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.693770027750958"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adb87a78a2fe46976f044c714d69c0d27f6a4c4223e919050b3edb2420e26cc9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
